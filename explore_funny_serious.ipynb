{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Funny vs. Serious Single Sentence Explorer\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davidbau/puns/blob/main/explore_funny_serious.ipynb)\n\n**Research question:** Does an LLM represent \"funny\" and \"serious\" sentence endings differently, even when both sentences are grammatically valid and make sense?\n\n## Dataset\n\nWe use the **funny_serious_150** dataset: 75 pairs of completed sentences where each pair has:\n- A **serious** version: `\"The dangerous iPhone was arrested and charged with assault.\"`\n- A **funny** version: `\"The dangerous iPhone was arrested and charged with battery.\"`\n\nBoth versions are valid English sentences. The only difference is the final word — one has the \"pun\" word, one has a straightforward word.\n\n## Method\n\nWe collect activations from Llama-3.1-70B-Instruct at the **final period** of each sentence, then analyze whether the model's internal representation differs based on whether it just processed a funny or serious completion.\n\n## Contents\n1. Dataset preview\n2. Collect activations (via NDIF)\n3. Load data and compute separation metrics\n4. Visualizations\n5. Cross-validation / holdout analysis\n6. Mean difference vs. Fisher LDA direction"
  },
  {
   "cell_type": "code",
   "id": "5g1la08wff4",
   "source": "# Colab Setup (run this cell first if on Google Colab)\n\nimport sys, os, subprocess\n\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    print(\"Running on Google Colab - setting up environment...\")\n\n    # Clone the repository\n    if not os.path.exists(\"puns\"):\n        subprocess.run([\"git\", \"clone\", \"https://github.com/davidbau/puns.git\"], check=True)\n    os.chdir(\"puns\")\n\n    # Install dependencies - use nnsight from git for latest NDIF compatibility\n    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \n                    \"git+https://github.com/ndif-team/nnsight.git\",\n                    \"torch\", \"numpy\", \"matplotlib\"], check=True)\n\n    # Create output directories\n    os.makedirs(\"results/raw_activations\", exist_ok=True)\n\n    # Note: nnsight will automatically read NDIF_API_KEY from Colab secrets\n    # To set up: click the key icon in the left sidebar, add a secret named \"NDIF_API_KEY\"\n    # Get your free API key at https://ndif.us/\n\n    print(\"Setup complete!\")\nelse:\n    print(\"Running locally - no setup needed.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "from IPython.display import HTML, display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "DATASET_FILE = Path(\"datasets/funny_serious_150.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Dataset Preview\n",
    "\n",
    "Let's look at a few example prompt pairs to understand the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATASET_FILE) as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "print(f\"Dataset: {len(dataset)} prompts ({len(dataset)//2} pairs)\\n\")\n",
    "print(\"Sample prompt pairs:\\n\")\n",
    "for i in range(0, min(10, len(dataset)), 2):\n",
    "    serious = dataset[i]\n",
    "    funny = dataset[i+1]\n",
    "    print(f\"Pair {serious['pair_id']}:\")\n",
    "    print(f\"  Serious: \\\"{serious['prompt']}\\\"\")\n",
    "    print(f\"  Funny:   \\\"{funny['prompt']}\\\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Collect Activations (via NDIF)\n",
    "\n",
    "We collect hidden-state activations from Llama-3.1-70B-Instruct using [NDIF](https://ndif.us/) (remote GPU inference). This cell checks if data already exists; if not, it runs the collection with progress display.\n",
    "\n",
    "**Note:** First-time collection requires an NDIF API key in `.env.local` and will run for several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collect_activations import ensure_activations\n",
    "\n",
    "# This checks if data exists; if not, collects via NDIF with progress display\n",
    "result = ensure_activations(\"funny_serious_150.json\", position=\"pred_c\")\n",
    "\n",
    "META_FILE = result[\"meta_file\"]\n",
    "print(f\"\\nMeta file: {META_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Load Data and Compute Separation Metrics\n",
    "\n",
    "Now we load the activations and compute metrics to measure how well the model separates funny vs. serious sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "from analyze_activations import load_activations, get_pair_indices\n",
    "\n",
    "# Explicit-argument analysis functions (show exactly what data is needed)\n",
    "from analyze_activations import (\n",
    "    mean_difference,           # (X, is_group_a, is_group_b) → direction\n",
    "    fisher_lda_direction,      # (X, is_group_a, is_group_b, shrinkage) → direction (covariance-corrected)\n",
    "    compute_fisher_separation, # (X, is_group_a, is_group_b) → float\n",
    "    compute_cohens_d,          # (X, is_group_a, is_group_b, direction) → float\n",
    ")\n",
    "\n",
    "# For visualization\n",
    "from analyze_activations import contrastive_projection, holdout_analysis\n",
    "from puns_viz import make_layer_viz\n",
    "\n",
    "# Load activations: returns metadata, per-layer activations, and layer indices\n",
    "meta, layer_data, layer_indices = load_activations(META_FILE)\n",
    "\n",
    "# Extract the key arrays we need for analysis:\n",
    "#   pair_ids:    which pair each prompt belongs to (0-74)\n",
    "#   is_funny:    True for prompts with the pun completion  \n",
    "#   is_serious:  True for prompts with the serious completion\n",
    "pair_ids, is_funny, is_serious = get_pair_indices(meta)\n",
    "\n",
    "print(f\"Model: {meta['model']}\")\n",
    "print(f\"Layers: {len(layer_indices)} (indices 0-{layer_indices[-1]})\")\n",
    "print(f\"Hidden dimension: {meta['hidden_dim']}\")\n",
    "print(f\"Total prompts: {len(is_funny)}\")\n",
    "print(f\"  - Funny completions: {is_funny.sum()}\")\n",
    "print(f\"  - Serious completions: {is_serious.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### Separation metrics across layers\n",
    "\n",
    "We'll compute two metrics at each layer:\n",
    "\n",
    "**Fisher separation** = (distance between group means) / (average within-group spread)\n",
    "- Measures how well-separated the two groups are in the full high-dimensional space\n",
    "- Higher = better separation\n",
    "\n",
    "**Cohen's d** = (mean_funny - mean_serious) / pooled_std, along the \"contrastive direction\"\n",
    "- The contrastive direction is simply: mean(funny) - mean(serious), normalized\n",
    "- Cohen's d tells us the effect size along this optimal separating direction\n",
    "- Interpretation: 0.2 = small, 0.5 = medium, 0.8 = large, >1.0 = very large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics at each layer\n",
    "fisher_scores = []\n",
    "cohens_d_scores = []\n",
    "\n",
    "for layer_idx in layer_indices:\n",
    "    # Get activations for this layer: shape (150, 8192)\n",
    "    X = layer_data[layer_idx]\n",
    "    \n",
    "    # Fisher separation: uses the full activation vectors\n",
    "    fisher = compute_fisher_separation(X, is_funny, is_serious)\n",
    "    fisher_scores.append(fisher)\n",
    "    \n",
    "    # Contrastive direction: the normalized mean difference\n",
    "    direction = mean_difference(X, is_funny, is_serious)\n",
    "    \n",
    "    # Cohen's d: effect size along the contrastive direction\n",
    "    d = compute_cohens_d(X, is_funny, is_serious, direction)\n",
    "    cohens_d_scores.append(d)\n",
    "\n",
    "# Find peak layers\n",
    "peak_fisher_idx = np.argmax(fisher_scores)\n",
    "peak_cd_idx = np.argmax(cohens_d_scores)\n",
    "\n",
    "print(f\"Fisher separation peaks at layer {layer_indices[peak_fisher_idx]}: {fisher_scores[peak_fisher_idx]:.3f}\")\n",
    "print(f\"Cohen's d peaks at layer {layer_indices[peak_cd_idx]}: {cohens_d_scores[peak_cd_idx]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot both metrics across layers\n",
    "fig, ax1 = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "ax1.plot(layer_indices, fisher_scores, color='#E85D75', lw=2, label='Fisher separation')\n",
    "ax1.set_xlabel('Layer', fontsize=11)\n",
    "ax1.set_ylabel('Fisher separation', fontsize=11, color='#E85D75')\n",
    "ax1.tick_params(axis='y', labelcolor='#E85D75')\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(layer_indices, cohens_d_scores, color='#2EAD6B', lw=2, ls='--', label=\"Cohen's d\")\n",
    "ax2.set_ylabel(\"Cohen's d\", fontsize=11, color='#2EAD6B')\n",
    "ax2.tick_params(axis='y', labelcolor='#2EAD6B')\n",
    "ax2.spines['top'].set_visible(False)\n",
    "\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, fontsize=9, loc='upper left')\n",
    "ax1.set_title('Funny vs. Serious: Separation by Layer', fontsize=13, fontweight='bold')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Visualizations\n",
    "\n",
    "Let's look at the actual activations at the layer with highest Cohen's d.\n",
    "\n",
    "The \"contrastive projection\" shows:\n",
    "- **X-axis**: Projection onto the contrastive direction (mean_funny - mean_serious)\n",
    "- **Y-axis**: First principal component of the residual (what's left after removing the contrastive direction)\n",
    "\n",
    "Lines connect each pair (funny and serious versions of the same joke)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get activations at peak Cohen's d layer\n",
    "peak_layer = layer_indices[peak_cd_idx]\n",
    "X_peak = layer_data[peak_layer]\n",
    "\n",
    "# Compute 2D contrastive projection (uses meta internally for pair structure)\n",
    "X_proj, components, var_ratios = contrastive_projection(X_peak, meta, n_components=2)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Draw lines connecting each pair\n",
    "for pid in sorted(set(pair_ids)):\n",
    "    mask = pair_ids == pid\n",
    "    if mask.sum() == 2:\n",
    "        pts = X_proj[mask]\n",
    "        ax.plot(pts[:, 0], pts[:, 1], color='#888', alpha=0.4, lw=0.8, zorder=1)\n",
    "\n",
    "# Scatter points\n",
    "ax.scatter(X_proj[is_serious, 0], X_proj[is_serious, 1], c='#4A90D9', \n",
    "           s=40, alpha=0.7, label='Serious', edgecolors='white', lw=0.5, zorder=2)\n",
    "ax.scatter(X_proj[is_funny, 0], X_proj[is_funny, 1], c='#E85D75',\n",
    "           s=40, alpha=0.7, label='Funny', edgecolors='white', lw=0.5, zorder=2)\n",
    "\n",
    "ax.set_xlabel(f'Contrastive direction ({var_ratios[0]:.1%} variance)', fontsize=11)\n",
    "ax.set_ylabel(f'Residual PC1 ({var_ratios[1]:.1%} variance)', fontsize=11)\n",
    "ax.set_title(f'Layer {peak_layer}: Contrastive Projection (Cohen\\'s d = {cohens_d_scores[peak_cd_idx]:.2f})', \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i3gp4g787bm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D histogram: projection onto contrastive direction\n",
    "direction = mean_difference(X_peak, is_funny, is_serious)\n",
    "projections = X_peak @ direction\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.hist(projections[is_serious], bins=15, alpha=0.6, color='#4A90D9', label='Serious', edgecolor='white')\n",
    "ax.hist(projections[is_funny], bins=15, alpha=0.6, color='#E85D75', label='Funny', edgecolor='white')\n",
    "ax.set_xlabel('Projection onto contrastive direction', fontsize=11)\n",
    "ax.set_ylabel('Count', fontsize=11)\n",
    "ax.set_title(f'Layer {peak_layer}: 1D Projections (Cohen\\'s d = {cohens_d_scores[peak_cd_idx]:.2f})',\n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mjgc8tk5eqj",
   "metadata": {},
   "source": [
    "### Interactive 3D Layer Explorer\n",
    "\n",
    "- **Drag** to rotate\n",
    "- **Scroll/pinch** to zoom\n",
    "- **Shift-drag** to pan\n",
    "- **Layer slider** to move through all layers\n",
    "- **Normalize layers** checkbox: scales each layer to the same magnitude (activations grow through layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m7ve4ukeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: pred_file=False skips loading predictions because pun boost isn't meaningful\n",
    "# for this dataset - these are completed sentences, not cloze predictions.\n",
    "html = make_layer_viz(META_FILE, pred_file=False, width=900, height=600)\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Cross-Validation / Holdout Analysis\n",
    "\n",
    "The contrastive direction is computed from the same data we use to measure Cohen's d. This could overfit — we might find a direction that separates *these specific* prompts but wouldn't generalize.\n",
    "\n",
    "**Holdout analysis**: Split pairs into two groups, compute the contrastive direction from one group (training set), measure Cohen's d on the other (holdout set). If the cross-validated Cohen's d is close to the full-data Cohen's d, the separation is real (not overfit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run holdout analysis (splits pairs, trains direction on half, tests on other half)\n",
    "holdout = holdout_analysis(layer_data, meta, n_splits=2, seed=42)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(holdout['layer_indices'], holdout['cohens_d_full'], color='#2EAD6B', lw=2, label='Full data')\n",
    "ax.plot(holdout['layer_indices'], holdout['cohens_d_cv'], color='#E85D75', lw=2, ls='--', label='Cross-validated')\n",
    "ax.set_xlabel('Layer', fontsize=11)\n",
    "ax.set_ylabel(\"Cohen's d\", fontsize=11)\n",
    "ax.set_title(\"Full Data vs. Cross-Validated Cohen's d\", fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Report the gap at peak layer\n",
    "cv_at_peak = holdout['cohens_d_cv'][peak_cd_idx]\n",
    "full_at_peak = holdout['cohens_d_full'][peak_cd_idx]\n",
    "print(f\"\\nAt peak layer {peak_layer}:\")\n",
    "print(f\"  Full-data Cohen's d: {full_at_peak:.2f}\")\n",
    "print(f\"  Cross-validated:     {cv_at_peak:.2f}\")\n",
    "print(f\"  Ratio (CV/full):     {cv_at_peak/full_at_peak:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "srdrovzg50q",
   "metadata": {},
   "source": [
    "### Visualizing Holdout: Training Direction Applied to Holdout Points\n",
    "\n",
    "Let's visualize the holdout analysis at the peak layer. We compute the contrastive direction from the training set (even-numbered pairs) and project the holdout set (odd-numbered pairs) onto this direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f00e6pbokr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split pairs into train (even pair_ids) and holdout (odd pair_ids)\n",
    "samples = meta[\"samples\"]\n",
    "unique_pairs = sorted(set(s[\"pair_id\"] for s in samples))\n",
    "\n",
    "train_mask = np.array([s[\"pair_id\"] % 2 == 0 for s in samples])\n",
    "holdout_mask = ~train_mask\n",
    "\n",
    "X_train = X_peak[train_mask]\n",
    "X_holdout = X_peak[holdout_mask]\n",
    "\n",
    "is_funny_train = is_funny[train_mask]\n",
    "is_serious_train = is_serious[train_mask]\n",
    "is_funny_holdout = is_funny[holdout_mask]\n",
    "is_serious_holdout = is_serious[holdout_mask]\n",
    "\n",
    "# Compute direction from training set only\n",
    "d_train = mean_difference(X_train, is_funny_train, is_serious_train)\n",
    "\n",
    "# Project holdout points onto training direction\n",
    "proj_holdout = X_holdout @ d_train\n",
    "\n",
    "# Compute holdout Cohen's d\n",
    "cd_holdout = compute_cohens_d(X_holdout, is_funny_holdout, is_serious_holdout, d_train)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.hist(proj_holdout[is_serious_holdout], bins=12, alpha=0.6, color='#4A90D9', label='Serious (holdout)', edgecolor='white')\n",
    "ax.hist(proj_holdout[is_funny_holdout], bins=12, alpha=0.6, color='#E85D75', label='Funny (holdout)', edgecolor='white')\n",
    "ax.set_xlabel('Projection onto TRAINING contrastive direction', fontsize=11)\n",
    "ax.set_ylabel('Count', fontsize=11)\n",
    "ax.set_title(f'Layer {peak_layer}: Holdout Points on Training Direction (Cohen\\'s d = {cd_holdout:.2f})',\n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training set: {train_mask.sum()} samples ({train_mask.sum()//2} pairs)\")\n",
    "print(f\"Holdout set:  {holdout_mask.sum()} samples ({holdout_mask.sum()//2} pairs)\")\n",
    "print(f\"Holdout Cohen's d using training direction: {cd_holdout:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xvgzh2mtnrs",
   "metadata": {},
   "source": [
    "### 3D Holdout Visualization\n",
    "\n",
    "Interactive 3D view of holdout points, projected using the training direction at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i38vak719y",
   "metadata": {},
   "outputs": [],
   "source": [
    "from puns_viz import make_holdout_viz\n",
    "\n",
    "# Create 3D visualization of holdout points using training direction\n",
    "html_holdout = make_holdout_viz(layer_data, meta, train_mask, width=900, height=600)\n",
    "HTML(html_holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1w6khfmouou",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Mean Difference vs. Fisher LDA Direction\n",
    "\n",
    "The simple **mean difference** direction `d = mean(funny) - mean(serious)` doesn't account for within-class covariance. The **Fisher LDA direction** (Marks et al., 2024) uses the inverse within-class covariance to \"whiten\" the space:\n",
    "\n",
    "```\n",
    "d_lda = Σ_within⁻¹ @ (mean(funny) - mean(serious))\n",
    "```\n",
    "\n",
    "This can give a more robust direction when variance differs across dimensions. However, for high-dimensional data (8192 dims with only 150 samples), we need regularization (shrinkage toward identity) to make the covariance matrix invertible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mczy89sjs6q",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare mean difference vs LDA direction at peak layer\n",
    "d_mean = mean_difference(X_peak, is_funny, is_serious)\n",
    "d_lda = fisher_lda_direction(X_peak, is_funny, is_serious, shrinkage=0.1)\n",
    "\n",
    "# Cohen's d along each direction\n",
    "cd_mean = compute_cohens_d(X_peak, is_funny, is_serious, d_mean)\n",
    "cd_lda = compute_cohens_d(X_peak, is_funny, is_serious, d_lda)\n",
    "\n",
    "# Angle between the two directions\n",
    "cos_angle = np.dot(d_mean, d_lda)\n",
    "angle_deg = np.degrees(np.arccos(np.clip(np.abs(cos_angle), 0, 1)))\n",
    "\n",
    "print(f\"At layer {peak_layer}:\")\n",
    "print(f\"  Mean difference Cohen's d: {cd_mean:.2f}\")\n",
    "print(f\"  Fisher LDA Cohen's d:      {cd_lda:.2f}\")\n",
    "print(f\"  Angle between directions:  {angle_deg:.1f}°\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbiynl11ird",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation comparison: does LDA generalize better?\n",
    "holdout_mean = holdout_analysis(layer_data, meta, n_splits=2, seed=42, use_lda=False)\n",
    "holdout_lda = holdout_analysis(layer_data, meta, n_splits=2, seed=42, use_lda=True, shrinkage=0.1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(holdout_mean['layer_indices'], holdout_mean['cohens_d_cv'], \n",
    "        color='#2EAD6B', lw=2, label='Mean difference (CV)')\n",
    "ax.plot(holdout_lda['layer_indices'], holdout_lda['cohens_d_cv'], \n",
    "        color='#E85D75', lw=2, ls='--', label='Fisher LDA (CV)')\n",
    "ax.set_xlabel('Layer', fontsize=11)\n",
    "ax.set_ylabel(\"Cohen's d (cross-validated)\", fontsize=11)\n",
    "ax.set_title(\"Cross-Validated Cohen's d: Mean Difference vs. Fisher LDA\", fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Report at peak layer\n",
    "cv_mean_at_peak = holdout_mean['cohens_d_cv'][peak_cd_idx]\n",
    "cv_lda_at_peak = holdout_lda['cohens_d_cv'][peak_cd_idx]\n",
    "print(f\"\\nCross-validated at layer {peak_layer}:\")\n",
    "print(f\"  Mean difference: {cv_mean_at_peak:.2f}\")\n",
    "print(f\"  Fisher LDA:      {cv_lda_at_peak:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Puns (3.12)",
   "language": "python",
   "name": "puns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}